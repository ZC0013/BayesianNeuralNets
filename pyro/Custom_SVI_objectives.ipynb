{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro import optim\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.0.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "\n",
    "\n",
    "# Set matplotlib settings\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "\n",
    "# Standard linear regression - nothing fancy here\n",
    "# Dataset: Add a feature to capture the interaction between \"cont_africa\" and \"rugged\"\n",
    "df[\"cont_africa_x_rugged\"] = df[\"cont_africa\"] * df[\"rugged\"]\n",
    "data = torch.tensor(df[[\"cont_africa\", \"rugged\", \"cont_africa_x_rugged\", \"rgdppc_2000\"]].values,\n",
    "                        dtype=torch.float)\n",
    "x_data, y_data = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian linear regression\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean\n",
    "# AutoDiagonalNormal guide that models the distribution of unobserved parameters\n",
    "# in the model as a Gaussian with diagonal covariance, i.e. it assumes \n",
    "# that there is no correlation amongst the latent variables\n",
    "# Under the hood, this defines a guide that uses a Normal distribution\n",
    "# with learnable parameters corresponding to each sample statement in the model\n",
    "\n",
    "model = BayesianRegression(3, 1)\n",
    "guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 4.6074\n",
      "[iteration 0101] loss: 4.1388\n",
      "[iteration 0201] loss: 4.1059\n",
      "[iteration 0301] loss: 3.8348\n",
      "[iteration 0401] loss: 3.7966\n",
      "[iteration 0501] loss: 3.6876\n",
      "[iteration 0601] loss: 3.5647\n",
      "[iteration 0701] loss: 3.5466\n",
      "[iteration 0801] loss: 3.4873\n",
      "[iteration 0901] loss: 3.4300\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We first review the basic usage pattern of SVI objects in Pyro.\n",
    "We assume that the user has defined a model and a guide.\n",
    "The user then creates an optimizer and an SVI object:\n",
    "\n",
    "\"\"\"\n",
    "pyro.clear_param_store()\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)},{\"clip_norm\": 10.0})\n",
    "svi = SVI(model, guide, optimizer, loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "for j in range(1000):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_data, y_data)\n",
    "    # Loss scaling \n",
    "    # loss = svi.step(x_data, y_data) / N_data\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_L2_regularizer(my_parameters):\n",
    "    reg_loss = 0.0\n",
    "    for param in my_parameters:\n",
    "        reg_loss = reg_loss + param.pow(2.0).sum()\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uceemfe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyro\\util.py:200: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
      "{'obs'}\n",
      "  guide_vars - aux_vars - model_vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 13.0063\n",
      "[iteration 0101] loss: 12.0189\n",
      "[iteration 0201] loss: 11.3696\n",
      "[iteration 0301] loss: 10.3619\n",
      "[iteration 0401] loss: 9.5758\n",
      "[iteration 0501] loss: 8.7246\n",
      "[iteration 0601] loss: 8.1958\n",
      "[iteration 0701] loss: 7.6023\n",
      "[iteration 0801] loss: 7.1693\n",
      "[iteration 0901] loss: 6.6601\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If we want more control, we can directly manipulate the differentiable loss\n",
    "method of the various ELBO classes. \n",
    "For example, (assuming we know all the parameters in advance) \n",
    "this is equivalent to the previous code snippet:\n",
    "\"\"\"\n",
    "pyro.clear_param_store()\n",
    "model = BayesianRegression(3, 1)\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "# We actually need to trace the parameters of the guide not of the model\n",
    "trace = poutine.trace(guide).get_trace(x_data)\n",
    "params = [trace.nodes[name][\"value\"].unconstrained() for name in trace.param_nodes]\n",
    "\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "# compute loss\n",
    "for j in range(1000):\n",
    "    # calculate the loss and take a gradient step and add a custom loss\n",
    "    loss = loss_fn(model, guide,x_data,y_data) + my_custom_L2_regularizer(params)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the gradients after the backward step\n",
    "    nn.utils.clip_grad_value_(params, 5.0)\n",
    "    # take a step and zero the parameter gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor some models the loss gradient can explode during training,\\nleading to overflow and NaN values.\\nOne way to protect against this is with gradient clipping.\\nThe optimizers in pyro.optim take an optional dictionary of clip_args\\nwhich allows clipping either the gradient\\nnorm or the gradient value to fall within the given limit.\\nTo change the basic example above:\\n\\n- optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)})\\n+ optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)}, {\"clip_norm\": 10.0})\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For some models the loss gradient can explode during training,\n",
    "leading to overflow and NaN values.\n",
    "One way to protect against this is with gradient clipping.\n",
    "The optimizers in pyro.optim take an optional dictionary of clip_args\n",
    "which allows clipping either the gradient\n",
    "norm or the gradient value to fall within the given limit.\n",
    "To change the basic example above:\n",
    "\n",
    "- optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)})\n",
    "+ optimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)}, {\"clip_norm\": 10.0})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
