{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n In particular as long as we use a TraceGraph_ELBO loss,\\n Pyro will keep track of the dependency structure within the execution traces\\n of the model and guide and construct a surrogate objective that has all the unnecessary terms removed:\\n \\n svi = SVI(model, guide, optimizer, TraceGraph_ELBO())\\n\\n Note that leveraging this dependency information takes extra computations,\\n so TraceGraph_ELBO should only be used in the case where your model\\n has non-reparameterizable random variables; in most applications Trace_ELBO suffices.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutorial found here: https://pyro.ai/examples/svi_part_iii.html\n",
    "\"\"\"\n",
    " In particular as long as we use a TraceGraph_ELBO loss,\n",
    " Pyro will keep track of the dependency structure within the execution traces\n",
    " of the model and guide and construct a surrogate objective that has all the unnecessary terms removed:\n",
    " \n",
    " svi = SVI(model, guide, optimizer, TraceGraph_ELBO())\n",
    "\n",
    " Note that leveraging this dependency information takes extra computations,\n",
    " so TraceGraph_ELBO should only be used in the case where your model\n",
    " has non-reparameterizable random variables; in most applications Trace_ELBO suffices.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo take advantage of Rao-Blackwellization\\nthe user needs to explicitly mark the conditional independence.\\n\\n# mark conditional independence\\n# (assumed to be along the rightmost tensor dimension)\\nwith pyro.plate(\"foo\", data.size(-1)):\\n    ks = pyro.sample(\"k\", dist.Categorical(probs))\\n    pyro.sample(\"obs\", dist.Normal(locs[ks], scale),\\n                obs=data)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To take advantage of Rao-Blackwellization\n",
    "the user needs to explicitly mark the conditional independence.\n",
    "\n",
    "# mark conditional independence\n",
    "# (assumed to be along the rightmost tensor dimension)\n",
    "with pyro.plate(\"foo\", data.size(-1)):\n",
    "    ks = pyro.sample(\"k\", dist.Categorical(probs))\n",
    "    pyro.sample(\"obs\", dist.Normal(locs[ks], scale),\n",
    "                obs=data)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReducing Variance with Data-Dependent Baselines\\n\\nThere are several ways the user can instruct Pyro to use baselines in\\nthe context of stochastic variational inference. Since baselines can\\nbe attached to any non-reparameterizable random variable, the current\\nbaseline interface is at the level of the pyro.sample statement.\\nIn particular the baseline interface makes use of an argument baseline\\n, which is a dictionary that specifies baseline options. \\nNote that it only makes sense to specify baselines for sample\\nstatements within the guide (and not in the model).\\n\\nz = pyro.sample(\"z\", dist.Bernoulli(...),\\n                infer=dict(baseline={\\'use_decaying_avg_baseline\\': True,\\n                                     \\'baseline_beta\\': 0.95}))\\n                                     \\nWe can also use a neural network to compute the baseline e.g.: \\n\\nclass BaselineNN(nn.Module):\\n    def __init__(self, dim_input, dim_hidden):\\n        super(BaselineNN, self).__init__()\\n        self.linear = nn.Linear(dim_input, dim_hidden)\\n        # ... finish initialization ...\\n\\n    def forward(self, x):\\n        hidden = self.linear(x)\\n        # ... do more computations ...\\n        return baseline\\n        \\n        \\nThen, assuming the BaselineNN object baseline_module\\nhas been initialized somewhere else, in the guide we’ll have something like\\n\\ndef guide(x):  # here x is the current mini-batch of data\\n    pyro.module(\"my_baseline\", baseline_module)\\n    # ... other computations ...\\n    z = pyro.sample(\"z\", dist.Bernoulli(...),\\n                    infer=dict(baseline={\\'nn_baseline\\': baseline_module,\\n                                         \\'nn_baseline_input\\': x}))\\n                                         \\nNote that the baseline module needs to be registered with Pyro with a pyro.module\\ncall so that Pyro is aware of the trainable parameters within the module.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reducing Variance with Data-Dependent Baselines\n",
    "\n",
    "There are several ways the user can instruct Pyro to use baselines in\n",
    "the context of stochastic variational inference. Since baselines can\n",
    "be attached to any non-reparameterizable random variable, the current\n",
    "baseline interface is at the level of the pyro.sample statement.\n",
    "In particular the baseline interface makes use of an argument baseline\n",
    ", which is a dictionary that specifies baseline options. \n",
    "Note that it only makes sense to specify baselines for sample\n",
    "statements within the guide (and not in the model).\n",
    "\n",
    "z = pyro.sample(\"z\", dist.Bernoulli(...),\n",
    "                infer=dict(baseline={'use_decaying_avg_baseline': True,\n",
    "                                     'baseline_beta': 0.95}))\n",
    "                                     \n",
    "We can also use a neural network to compute the baseline e.g.: \n",
    "\n",
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden):\n",
    "        super(BaselineNN, self).__init__()\n",
    "        self.linear = nn.Linear(dim_input, dim_hidden)\n",
    "        # ... finish initialization ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.linear(x)\n",
    "        # ... do more computations ...\n",
    "        return baseline\n",
    "        \n",
    "        \n",
    "Then, assuming the BaselineNN object baseline_module\n",
    "has been initialized somewhere else, in the guide we’ll have something like\n",
    "\n",
    "def guide(x):  # here x is the current mini-batch of data\n",
    "    pyro.module(\"my_baseline\", baseline_module)\n",
    "    # ... other computations ...\n",
    "    z = pyro.sample(\"z\", dist.Bernoulli(...),\n",
    "                    infer=dict(baseline={'nn_baseline': baseline_module,\n",
    "                                         'nn_baseline_input': x}))\n",
    "                                         \n",
    "Note that the baseline module needs to be registered with Pyro with a pyro.module\n",
    "call so that Pyro is aware of the trainable parameters within the module.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "# Pyro also has a reparameterized Beta distribution so we import\n",
    "# the non-reparameterized version to make our point\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "import sys\n",
    "\n",
    "# enable validation (e.g. validate parameters of distributions)\n",
    "assert pyro.__version__.startswith('1.0.0')\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBetaExample(object):\n",
    "    def __init__(self, max_steps):\n",
    "        # the maximum number of inference steps we do\n",
    "        self.max_steps = max_steps\n",
    "        # the two hyperparameters for the beta prior\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        # the dataset consists of six 1s and four 0s\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        # compute the alpha parameter of the exact beta posterior\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        # compute the beta parameter of the exact beta posterior\n",
    "        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "        # initial values of the two variational parameters\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        # sample `latent_fairness` from the beta prior\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        # use plate to indicate that the observations are\n",
    "        # conditionally independent given f and get vectorization\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            # observe all ten datapoints using the bernoulli likelihood\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        # register the two variational parameters with pyro\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0),\n",
    "                             constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0),\n",
    "                            constraint=constraints.positive)\n",
    "        # sample f from the beta variational distribution\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "                         'baseline_beta': 0.90}\n",
    "        # note that the baseline_dict specifies whether we're using\n",
    "        # decaying average baselines or not\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q),\n",
    "                    infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        # clear the param store in case we're in a REPL\n",
    "        pyro.clear_param_store()\n",
    "        # setup the optimizer and the inference algorithm\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        # do up to this many steps of inference\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # compute the distance to the parameters of the true posterior\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            # stop inference early if we're close to the true posterior\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 106 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7987 & 0.7732\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      "........\n",
      "Did 724 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7974 & 0.7822\n"
     ]
    }
   ],
   "source": [
    "bbe = BernoulliBetaExample(max_steps=max_steps)\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
