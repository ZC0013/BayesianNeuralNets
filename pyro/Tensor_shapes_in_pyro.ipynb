{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Original tutorial: http://pyro.ai/examples/tensor_shapes.html\n",
    "\n",
    "While you are learning or debugging, set pyro.enable_validation(True).\n",
    "Tensors broadcast by aligning on the right: torch.ones(3,4,5) + torch.ones(5).\n",
    "Distribution .sample().shape == batch_shape + event_shape.\n",
    "Distribution .log_prob(x).shape == batch_shape (but not event_shape!).\n",
    "Use .expand() to draw a batch of samples, or rely on plate to expand automatically.\n",
    "Use my_dist.to_event(1) to declare a dimension as dependent.\n",
    "Use with pyro.plate('name', size): to declare a dimension as conditionally independent.\n",
    "All dimensions must be declared either dependent or conditionally independent.\n",
    "Try to support batching on the left. This lets Pyro auto-parallelize.\n",
    "use negative indices like x.sum(-1) rather than x.sum(2)\n",
    "use ellipsis notation like pixel = image[..., i, j]\n",
    "use Vindex if i,j are enumerated, pixel = Vindex(image)[..., i, j]\n",
    "When debugging, examine all shapes in a trace using Trace.format_shapes().\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import os\n",
    "from torch.distributions import constraints\n",
    "from pyro.distributions import Bernoulli, Categorical, MultivariateNormal, Normal\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "from pyro.infer import Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "from pyro.optim import Adam\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.0.0')\n",
    "pyro.enable_validation(True)    # <---- This is always a good idea!\n",
    "\n",
    "# We'll ue this helper to check our models are correct.\n",
    "def test_model(model, guide, loss):\n",
    "    pyro.clear_param_store()\n",
    "    loss.loss(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch Tensors have a single .shape attribute, \n",
    "but Distributions have two shape attributions with special meaning: \n",
    ".batch_shape and .event_shape. These two combine to define the total shape of a sample\n",
    "\n",
    "Indices over .batch_shape denote conditionally independent random variables, \n",
    "whereas indices over .event_shape denote dependent random variables (ie one draw from a distribution)\n",
    "\n",
    "Note that the Distribution.sample() method also takes a sample_shape parameter\n",
    "that indexes over independent identically distributed (iid) random varables, so that\n",
    "\n",
    "x2.shape == sample_shape + batch_shape + event_shape\n",
    "\n",
    "Meaning that we can have multiple samples for each item in the batch \n",
    "\"\"\"\n",
    "d = Bernoulli(0.5)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == ()\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 1.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 0.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [0., 1., 0., 0.]]])\n",
      "tensor([[[-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931]],\n",
      "\n",
      "        [[-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931]],\n",
      "\n",
      "        [[-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931]],\n",
      "\n",
      "        [[-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931]],\n",
      "\n",
      "        [[-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931],\n",
      "         [-0.6931, -0.6931, -0.6931, -0.6931]]])\n"
     ]
    }
   ],
   "source": [
    "d = Bernoulli(0.5 * torch.ones(3,4)) # If this particular tensor or in general if tensor is passed as an initializer that is the resulttant distribution shape\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample((5,))\n",
    "print(x)\n",
    "assert x.shape == (5,3, 4)\n",
    "assert d.log_prob(x).shape == (5,3, 4)\n",
    "print(d.log_prob(x))\n",
    "# Which is ln(0.5) == -0.6931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDistributions over vectors like MultivariateNormal have len(event_shape) == 1.\\nDistributions over matrices like InverseWishart have len(event_shape) == 2\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Distributions over vectors like MultivariateNormal have len(event_shape) == 1.\n",
    "Distributions over matrices like InverseWishart have len(event_shape) == 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[-0.1054, -0.2231, -1.2040, -0.5108],\n",
      "        [-0.1054, -0.2231, -0.3567, -0.5108],\n",
      "        [-0.1054, -0.2231, -0.3567, -0.9163]])\n"
     ]
    }
   ],
   "source": [
    "d = Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).expand([3, 4])\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)\n",
    "print(x)\n",
    "print(d.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4929, 0.0339, 0.2949])\n",
      "tensor(-2.9223)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multivariate distributions have nonempty .event_shape. \n",
    "For these distributions, the shapes of .sample() and .log_prob(x) differ:\n",
    "\"\"\"\n",
    "d = MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,)\n",
    "x = d.sample()\n",
    "print(x)\n",
    "assert x.shape == (3,)            # == batch_shape + event_shape\n",
    "assert d.log_prob(x).shape == ()  # == batch_shape\n",
    "print(d.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor(-8.3178)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In Pyro you can treat a univariate distribution as multivariate by calling the .to_event(n)\n",
    "property where n is the number of batch dimensions (from the right) to declare as dependent.\n",
    "\"\"\"\n",
    "d = Bernoulli(0.5 * torch.ones(3,4)).to_event(2)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,4)\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == ()\n",
    "print(x)\n",
    "print(d.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhile you work with Pyro programs, keep in mind that samples have shape batch_shape + event_shape,\\nwhereas .log_prob(x) values have shape batch_shape. \\nYou’ll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n)\\nor by declaring dimensions as independent via pyro.plate.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "While you work with Pyro programs, keep in mind that samples have shape batch_shape + event_shape,\n",
    "whereas .log_prob(x) values have shape batch_shape. \n",
    "You’ll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n)\n",
    "or by declaring dimensions as independent via pyro.plate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TorchDistributionMixin.shape of Independent()> torch.Size([]) torch.Size([10])\n",
      "tensor([ 0.9060, -0.2277,  0.4350, -0.1837,  0.6944, -2.0546, -0.4330,  0.1646,\n",
      "         1.5989, -1.5854])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent, e.g.\n",
    "\"\"\"\n",
    "d = Normal(0, 1).expand([10]).to_event(1)\n",
    "print(d.shape,d.batch_shape, d.event_shape)\n",
    "x = d.sample()\n",
    "print(x, )\n",
    "assert x.shape == (10,)\n",
    "\"\"\"\n",
    "This is useful for two reasons: First it allows us to easily swap in a MultivariateNormal distribution later.\n",
    "Second it simplifies the code a bit since we don’t need a plate as in\n",
    "\"\"\"\n",
    "with pyro.plate(\"x_plate\", 10):\n",
    "    x = pyro.sample(\"x\", Normal(0, 1))  # .expand([10]) is automatic\n",
    "    assert x.shape == (10,)\n",
    "    \n",
    "\"\"\"\n",
    "The difference between these two versions is that the second version with plate\n",
    "informs Pyro that it can make use of conditional independence information when estimating gradients,\n",
    "whereas in the first version Pyro must assume they are dependent\n",
    "(even though the normals are in fact conditionally independent).\n",
    "This is analogous to d-separation in graphical models: it is always safe to add edges\n",
    "and assume variables may be dependent (i.e. to widen the model class), \n",
    "but it is unsafe to assume independence when variables are actually dependent\n",
    "(i.e. narrowing the model class so the true model lies outside of the class,\n",
    "as in mean field). \n",
    "\n",
    "In practice Pyro’s SVI inference algorithm uses\n",
    "reparameterized gradient estimators for Normal distributions\n",
    "so both gradient estimators have the same performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We recommend always providing an optional size argument to aid in debugging shapes\n",
    "\n",
    "\"\"\"\n",
    "with pyro.plate(\"my_plate\", 10):\n",
    "    pass\n",
    "    # within this context, batch dimension -1 is independent\n",
    "\"\"\"\n",
    "Starting with Pyro 0.2 you can additionally nest plates, e.g. if you have per-pixel independence:\n",
    "\n",
    "\"\"\"\n",
    "with pyro.plate(\"x_axis\", 320):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "    with pyro.plate(\"y_axis\", 200):\n",
    "        # within this context, batch dimensions -2 and -1 are independent\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:            \n",
      " Param Sites:            \n",
      "Sample Sites:            \n",
      "       a dist       |    \n",
      "        value       |    \n",
      "     log_prob       |    \n",
      "       b dist       | 2  \n",
      "        value       | 2  \n",
      "     log_prob       |    \n",
      " c_plate dist       |    \n",
      "        value     2 |    \n",
      "     log_prob       |    \n",
      "       c dist     2 |    \n",
      "        value     2 |    \n",
      "     log_prob     2 |    \n",
      " d_plate dist       |    \n",
      "        value     3 |    \n",
      "     log_prob       |    \n",
      "       d dist     3 | 4 5\n",
      "        value     3 | 4 5\n",
      "     log_prob     3 |    \n",
      "  x_axis dist       |    \n",
      "        value     3 |    \n",
      "     log_prob       |    \n",
      "  y_axis dist       |    \n",
      "        value     2 |    \n",
      "     log_prob       |    \n",
      "       x dist   3 1 |    \n",
      "        value   3 1 |    \n",
      "     log_prob   3 1 |    \n",
      "       y dist 2 1 1 |    \n",
      "        value 2 1 1 |    \n",
      "     log_prob 2 1 1 |    \n",
      "      xy dist 2 3 1 |    \n",
      "        value 2 3 1 |    \n",
      "     log_prob 2 3 1 |    \n",
      "       z dist 2 3 1 | 5  \n",
      "        value 2 3 1 | 5  \n",
      "     log_prob 2 3 1 |    \n"
     ]
    }
   ],
   "source": [
    "def model1():\n",
    "    a = pyro.sample(\"a\", Normal(0, 1))\n",
    "    b = pyro.sample(\"b\", Normal(torch.zeros(2), 1).to_event(1))\n",
    "    with pyro.plate(\"c_plate\", 2):\n",
    "        c = pyro.sample(\"c\", Normal(torch.zeros(2), 1))\n",
    "    with pyro.plate(\"d_plate\", 3):\n",
    "        d = pyro.sample(\"d\", Normal(torch.zeros(3,4,5), 1).to_event(2))\n",
    "    assert a.shape == ()       # batch_shape == ()     event_shape == ()\n",
    "    assert b.shape == (2,)     # batch_shape == ()     event_shape == (2,)\n",
    "    assert c.shape == (2,)     # batch_shape == (2,)   event_sahpe == ()\n",
    "    assert d.shape == (3,4,5)  # batch_shape == (3,)   event_shape == (4,5)\n",
    "\n",
    "    x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
    "    y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
    "    with x_axis:\n",
    "        x = pyro.sample(\"x\", Normal(0, 1))\n",
    "    with y_axis:\n",
    "        y = pyro.sample(\"y\", Normal(0, 1))\n",
    "    with x_axis, y_axis:\n",
    "        xy = pyro.sample(\"xy\", Normal(0, 1))\n",
    "        z = pyro.sample(\"z\", Normal(0, 1).expand([5]).to_event(1))\n",
    "    assert x.shape == (3, 1)        # batch_shape == (3,1)     event_shape == ()\n",
    "    assert y.shape == (2, 1, 1)     # batch_shape == (2,1,1)   event_shape == ()\n",
    "    assert xy.shape == (2, 3, 1)    # batch_shape == (2,3,1)   event_shape == ()\n",
    "    assert z.shape == (2, 3, 1, 5)  # batch_shape == (2,3,1)   event_shape == (5,)\n",
    "\n",
    "test_model(model1, model1, Trace_ELBO())\n",
    "trace = poutine.trace(model1).get_trace()\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One of the main uses of plate is to subsample data. \n",
    "This is possible within a plate because data are conditionally independent, \n",
    "so the expected value of the loss on, say,\n",
    "half the data should be half the expected loss on the full data.\n",
    "\n",
    "To subsample data, you need to inform Pyro of both the original data size\n",
    "and the subsample size; Pyro will then choose a random subset\n",
    "of data and yield the set of indices.\n",
    "\"\"\"\n",
    "data = torch.arange(100.)\n",
    "\n",
    "def model2():\n",
    "    mean = pyro.param(\"mean\", torch.zeros(len(data)))\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=10) as ind:\n",
    "        assert len(ind) == 10    # ind is a LongTensor that indexes the subsample.\n",
    "        batch = data[ind]        # Select a minibatch of data.\n",
    "        mean_batch = mean[ind]   # Take care to select the relevant per-datum parameters.\n",
    "        # Do stuff with batch:\n",
    "        x = pyro.sample(\"x\", Normal(mean_batch, 1), obs=batch)\n",
    "        assert len(x) == 10\n",
    "\n",
    "test_model(model2, guide=lambda: None, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pyro 0.2 introduces the ability to enumerate discrete latent variables in parallel. \n",
    "This can significantly reduce the variance of gradient estimators when learning a posterior via SVI.\n",
    "\"\"\"\n",
    "@config_enumerate\n",
    "def model3():\n",
    "    p = pyro.param(\"p\", torch.arange(6.) / 6)\n",
    "    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n",
    "\n",
    "    a = pyro.sample(\"a\", Categorical(torch.ones(6) / 6))\n",
    "    b = pyro.sample(\"b\", Bernoulli(p[a]))  # Note this depends on a.\n",
    "    with pyro.plate(\"c_plate\", 4):\n",
    "        c = pyro.sample(\"c\", Bernoulli(0.3))\n",
    "        with pyro.plate(\"d_plate\", 5):\n",
    "            d = pyro.sample(\"d\", Bernoulli(0.4))\n",
    "            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            e_scale = torch.arange(1., 8.)\n",
    "            e = pyro.sample(\"e\", Normal(e_loc, e_scale)\n",
    "                            .to_event(1))  # Note this depends on d.\n",
    "\n",
    "    #                   enumerated|batch|event dims\n",
    "    assert a.shape == (         6, 1, 1   )  # Six enumerated values of the Categorical.\n",
    "    assert b.shape == (      2, 1, 1, 1   )  # Two enumerated Bernoullis, unexpanded.\n",
    "    assert c.shape == (   2, 1, 1, 1, 1   )  # Only two Bernoullis, unexpanded.\n",
    "    assert d.shape == (2, 1, 1, 1, 1, 1   )  # Only two Bernoullis, unexpanded.\n",
    "    assert e.shape == (2, 1, 1, 1, 5, 4, 7)  # This is sampled and depends on d.\n",
    "\n",
    "    assert e_loc.shape   == (2, 1, 1, 1, 1, 1, 1,)\n",
    "    assert e_scale.shape == (                  7,)\n",
    "\n",
    "test_model(model3, model3, TraceEnum_ELBO(max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "height = 10\n",
    "sparse_pixels = torch.LongTensor([[3, 2], [3, 5], [3, 9], [7, 1]])\n",
    "enumerated = None  # set to either True or False below\n",
    "\n",
    "def fun(observe):\n",
    "    p_x = pyro.param(\"p_x\", torch.tensor(0.1), constraint=constraints.unit_interval)\n",
    "    p_y = pyro.param(\"p_y\", torch.tensor(0.1), constraint=constraints.unit_interval)\n",
    "    x_axis = pyro.plate('x_axis', width, dim=-2)\n",
    "    y_axis = pyro.plate('y_axis', height, dim=-1)\n",
    "\n",
    "    # Note that the shapes of these sites depend on whether Pyro is enumerating.\n",
    "    with x_axis:\n",
    "        x_active = pyro.sample(\"x_active\", Bernoulli(p_x))\n",
    "    with y_axis:\n",
    "        y_active = pyro.sample(\"y_active\", Bernoulli(p_y))\n",
    "    if enumerated:\n",
    "        assert x_active.shape  == (2, 1, 1)\n",
    "        assert y_active.shape  == (2, 1, 1, 1)\n",
    "    else:\n",
    "        assert x_active.shape  == (width, 1)\n",
    "        assert y_active.shape  == (height,)\n",
    "\n",
    "    # The first trick is to broadcast. This works with or without enumeration.\n",
    "    p = 0.1 + 0.5 * x_active * y_active\n",
    "    if enumerated:\n",
    "        assert p.shape == (2, 2, 1, 1)\n",
    "    else:\n",
    "        assert p.shape == (width, height)\n",
    "    dense_pixels = p.new_zeros(broadcast_shape(p.shape, (width, height)))\n",
    "\n",
    "    # The second trick is to index using ellipsis slicing.\n",
    "    # This allows Pyro to add arbitrary dimensions on the left.\n",
    "    for x, y in sparse_pixels:\n",
    "        dense_pixels[..., x, y] = 1\n",
    "    if enumerated:\n",
    "        assert dense_pixels.shape == (2, 2, width, height)\n",
    "    else:\n",
    "        assert dense_pixels.shape == (width, height)\n",
    "\n",
    "    with x_axis, y_axis:\n",
    "        if observe:\n",
    "            pyro.sample(\"pixels\", Bernoulli(p), obs=dense_pixels)\n",
    "\n",
    "def model4():\n",
    "    fun(observe=True)\n",
    "\n",
    "def guide4():\n",
    "    fun(observe=False)\n",
    "\n",
    "# Test without enumeration.\n",
    "enumerated = False\n",
    "test_model(model4, guide4, Trace_ELBO())\n",
    "\n",
    "# Test with enumeration.\n",
    "enumerated = True\n",
    "test_model(model4, config_enumerate(guide4, \"parallel\"),\n",
    "           TraceEnum_ELBO(max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
