{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tutorial https://pyro.ai/examples/ss-vae.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenges of Inference\n",
    "For concreteness we’re going to continue to assume that the partially-observed {yi} are discrete labels; we will also assume that the {zi} are continuous.\n",
    "\n",
    "If we apply the general recipe for stochastic variational inference to our model (see SVI Part I) we would be sampling the discrete (and thus non-reparameterizable) variable yi whenever it’s unobserved. As discussed in SVI Part III this will generally lead to high-variance gradient estimates.\n",
    "A common way to ameliorate this problem—and one that we’ll explore below—is to forego sampling and instead sum out all ten values of the class label yi when we calculate the ELBO for an unlabeled datapoint xi. This is more expensive per step, but can help us reduce the variance of our gradient estimator and thereby take fewer steps.\n",
    "Recall that the role of the guide is to ‘fill in’ latent random variables. Concretely, one component of our guide will be a digit classifier qϕ(y|x) that will randomly ‘fill in’ labels {yi} given an image {xi}. Crucially, this means that the only term in the ELBO that will depend on qϕ(⋅|x) is the term that involves a sum over unlabeled datapoints. This means that our classifier qϕ(⋅|x)—which in many cases will be the primary object of interest—will not be learning from the labeled datapoints (at least not directly).\n",
    "This seems like a potential problem. Luckily, various fixes are possible. Below we’ll follow the approach in reference [1], which involves introducing an additional objective function for the classifier to ensure that the classifier learns directly from the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(self, xs, ys=None):\n",
    "    # register this pytorch module and all of its sub-modules with pyro\n",
    "    pyro.module(\"ss_vae\", self)\n",
    "    batch_size = xs.size(0)\n",
    "\n",
    "    # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "    with pyro.plate(\"data\"):\n",
    "\n",
    "        # sample the handwriting style from the constant prior distribution\n",
    "        prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "        prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "        zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "        # if the label y (which digit to write) is supervised, sample from the\n",
    "        # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "        alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "        ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "\n",
    "        # finally, score the image (x) using the handwriting style (z) and\n",
    "        # the class label y (which digit to write) against the\n",
    "        # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "        # where `decoder` is a neural network\n",
    "        loc = self.decoder.forward([zs, ys])\n",
    "        pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "\n",
    "def guide(self, xs, ys=None):\n",
    "    with pyro.plate(\"data\"):\n",
    "        # if the class label (the digit) is not supervised, sample\n",
    "        # (and score) the digit with the variational distribution\n",
    "        # q(y|x) = categorical(alpha(x))\n",
    "        if ys is None:\n",
    "            alpha = self.encoder_y.forward(xs)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "\n",
    "        # sample (and score) the latent handwriting-style with the variational\n",
    "        # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "        loc, scale = self.encoder_z.forward([xs, ys])\n",
    "        pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sum out all the disrete variables: \n",
    "svi = SVI(model, config_enumerate(guide), optimizer, loss=TraceEnum_ELBO(max_plate_nesting=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
