{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tutorial: https://pyro.ai/examples/bayesian_regression_ii.html\n",
    "\n",
    "%reset -sf\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive, MCMC, NUTS\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal, init_to_mean\n",
    "\n",
    "\n",
    "\n",
    "pyro.set_rng_seed(1)\n",
    "assert pyro.__version__.startswith('1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "# Enable validation checks\n",
    "pyro.enable_validation(True)\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.set_rng_seed(1)\n",
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "rugged_data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly write out each parameter in the Guide or in the Model\n",
    "\n",
    "def model(is_cont_africa, ruggedness, log_gdp):\n",
    "    a = pyro.sample(\"a\", dist.Normal(0., 10.))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(0., 1.))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(0., 1.))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(0., 1.))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness\n",
    "    with pyro.plate(\"data\", len(ruggedness)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=log_gdp)\n",
    "\n",
    "def guide(is_cont_africa, ruggedness, log_gdp):\n",
    "    a_loc = pyro.param('a_loc', torch.tensor(0.))\n",
    "    a_scale = pyro.param('a_scale', torch.tensor(1.),\n",
    "                         constraint=constraints.positive)\n",
    "    sigma_loc = pyro.param('sigma_loc', torch.tensor(1.),\n",
    "                             constraint=constraints.positive)\n",
    "    \n",
    "    sigma_scale = pyro.param('sigma_scale', torch.tensor(0.05),\n",
    "                             constraint=constraints.positive)\n",
    "    weights_loc = pyro.param('weights_loc', torch.randn(3))\n",
    "    weights_scale = pyro.param('weights_scale', torch.ones(3),\n",
    "                               constraint=constraints.positive)\n",
    "    a = pyro.sample(\"a\", dist.Normal(a_loc, a_scale))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Normal(sigma_loc, sigma_scale))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to print latent sites' quantile information.\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "# Prepare training data\n",
    "df = rugged_data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "train = torch.tensor(df.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 5795.467590510845\n",
      "Elbo loss: 415.8889375925064\n",
      "Elbo loss: 250.01099556684494\n",
      "Elbo loss: 247.14538943767548\n",
      "Elbo loss: 249.2295293211937\n",
      "Elbo loss: 250.9435819387436\n",
      "Elbo loss: 249.57079249620438\n",
      "Elbo loss: 248.7939082980156\n",
      "Elbo loss: 248.6645919084549\n",
      "Elbo loss: 250.3539196252823\n"
     ]
    }
   ],
   "source": [
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({\"lr\": .05}),\n",
    "          loss=Trace_ELBO())\n",
    "\n",
    "is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2]\n",
    "pyro.clear_param_store()\n",
    "num_iters = 5000 if not smoke_test else 2\n",
    "for i in range(num_iters):\n",
    "    elbo = svi.step(is_cont_africa, ruggedness, log_gdp)\n",
    "    if i % 500 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "svi_samples = {k: v.reshape(num_samples).detach().cpu().numpy()\n",
    "               for k, v in predictive(log_gdp, is_cont_africa, ruggedness).items()\n",
    "               if k != \"obs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: a\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  9.177097  0.059518  9.078332  9.140592  9.178284  9.217113  9.271454 \n",
      "\n",
      "Site: bA\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -1.890339  0.122807 -2.088208 -1.978823 -1.887191 -1.803397 -1.700566 \n",
      "\n",
      "Site: bR\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -0.157294  0.039547 -0.222703 -0.183125 -0.157319 -0.132543 -0.091143 \n",
      "\n",
      "Site: bAR\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  0.304762  0.067878  0.194515  0.259582  0.305156  0.349307  0.415692 \n",
      "\n",
      "Site: sigma\n",
      "      mean       std        5%       25%       50%       75%       95%\n",
      "0  0.90384  0.048398  0.824407  0.870968  0.902914  0.936399  0.983218 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site, values in summary(svi_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1200/1200 [00:29, 40.18it/s, step size=3.57e-01, acc. prob=0.888]\n"
     ]
    }
   ],
   "source": [
    "# we can also do exact inference with MCMC \n",
    "nuts_kernel = NUTS(model)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
    "mcmc.run(is_cont_africa, ruggedness, log_gdp)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: a\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  9.186106  0.144717  8.951578  9.091258  9.185141  9.274197  9.420919 \n",
      "\n",
      "Site: bA\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -1.845644  0.218941 -2.218035 -1.982632 -1.843927 -1.709069 -1.491678 \n",
      "\n",
      "Site: bR\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0 -0.186122  0.078746 -0.310428 -0.243365 -0.185347 -0.131482 -0.058996 \n",
      "\n",
      "Site: bAR\n",
      "       mean       std        5%       25%       50%      75%       95%\n",
      "0  0.351446  0.122918  0.152581  0.272872  0.349664  0.43567  0.553147 \n",
      "\n",
      "Site: sigma\n",
      "      mean       std        5%       25%       50%       75%     95%\n",
      "0  0.95116  0.051794  0.868952  0.916505  0.947113  0.983869  1.0391 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site, values in summary(hmc_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 707.9290481805801\n",
      "Elbo loss: 426.1432435512543\n",
      "Elbo loss: 259.178986787796\n",
      "Elbo loss: 249.957903444767\n",
      "Elbo loss: 247.92973798513412\n",
      "Elbo loss: 247.04828417301178\n",
      "Elbo loss: 247.64889878034592\n",
      "Elbo loss: 247.19207900762558\n",
      "Elbo loss: 250.07714819908142\n",
      "Elbo loss: 249.06675231456757\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "As comparison to the previously obtained results from Diagonal Normal guide,\n",
    "we will now use a guide that generates samples from a Cholesky factorization\n",
    "of a multivariate normal distribution. This allows us to capture the correlations\n",
    "between the latent variables via a covariance matrix. If we wrote this manually,\n",
    "we would need to combine all the latent variables so we could sample a Multivarite Normal jointly.\n",
    "\n",
    "\"\"\"\n",
    "guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)\n",
    "\n",
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({\"lr\": .01}),\n",
    "          loss=Trace_ELBO())\n",
    "\n",
    "is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2]\n",
    "pyro.clear_param_store()\n",
    "for i in range(num_iters):\n",
    "    elbo = svi.step(is_cont_africa, ruggedness, log_gdp)\n",
    "    if i % 500 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(elbo))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
