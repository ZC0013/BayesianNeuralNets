{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and some utility functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pyro.distributions.torch import Normal\n",
    "from pyro.distributions.torch import Categorical\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# If GPU is available set it to a GPU\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import numpy as np\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.0,),(1/255.0,)) \n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Get TRAINING and VALIDATION datasets\n",
    "# Define transforms\n",
    "valid_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "train_dataset = datasets.MNIST(root=data_dir, train=True, \n",
    "                download=True, transform=train_transform)\n",
    "\n",
    "valid_dataset = datasets.MNIST(root=data_dir, train=True, \n",
    "                download=True, transform=valid_transform)\n",
    "\n",
    "# Get 20% as a validation dataset\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.2 * num_train))\n",
    "\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                batch_size=64, sampler=train_sampler, \n",
    "                num_workers=8)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                batch_size=64, sampler=valid_sampler, \n",
    "                num_workers=8)\n",
    "\n",
    "    # define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST(root=data_dir, \n",
    "                           train=False, \n",
    "                           download=True,\n",
    "                           transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                        batch_size=32, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=8)\n",
    "\n",
    "examples = enumerate(train_loader)\n",
    "_, (image, label) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.tight_layout()\n",
    "    _, (image, label) = next(examples)\n",
    "    i = np.random.randint(0,63)\n",
    "    plt.imshow(torch.squeeze(image[i]), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(label[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a very simple model consisting of 4 inputs, 10 input nodes and 4 output nodes\n",
    "class FCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.input = torch.nn.Linear(4, 10)\n",
    "        self.output = torch.nn.Linear(10,4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "net = FCNet()\n",
    "net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PRIORS for the model that we have just defined\n",
    "def model(x_data, y_data):\n",
    "    # Define normal priors for INPUT layer\n",
    "    inputw_prior = Normal(loc=torch.zeros_like(net.input.weight), scale=torch.ones_like(net.input.weight))\n",
    "    inputb_prior = Normal(loc=torch.zeros_like(net.input.bias), scale=torch.ones_like(net.input.bias))\n",
    "    \n",
    "    # Define normal priors for OUTPUT layer\n",
    "    outputw_prior = Normal(loc=torch.zeros_like(net.output.weight), scale=torch.ones_like(net.output.weight))\n",
    "    outputb_prior = Normal(loc=torch.zeros_like(net.output.bias), scale=torch.ones_like(net.output.bias))\n",
    "    \n",
    "    # \n",
    "    priors = {'input.weight': inputw_prior, 'input.bias': inputb_prior,  'output.weight': outputw_prior, 'output.bias': outputb_prior}\n",
    "    \n",
    "    # Lift module parameters to \n",
    "    # random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    # Sample a regressor to sample w and b\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    # Pyro does not have an automatic activation depending on loss and given\n",
    "    # That we want to do classification we need to use log softmax\n",
    "    lhat = F.log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is going to represent the POSTERIOR\n",
    "# that we have just defined and which we are going try\n",
    "# and approximate to the true posterior\n",
    "def guide(x_data, y_data):\n",
    "    # First layer weight distribution priors\n",
    "    inputw_mu = torch.randn_like(net.input.weight)\n",
    "    inputw_sigma = torch.randn_like(net.input.weight)\n",
    "    inputw_mu_param = pyro.param(\"inputw_mu\", inputw_mu)\n",
    "    inputw_sigma_param = F.softplus(pyro.param(\"inputw_sigma\", inputw_sigma))\n",
    "    inputw_prior = Normal(loc=inputw_mu_param, scale=inputw_sigma_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    inputb_mu = torch.randn_like(net.input.bias)\n",
    "    inputb_sigma = torch.randn_like(net.input.bias)\n",
    "    inputb_mu_param = pyro.param(\"inputb_mu\", inputb_mu)\n",
    "    inputb_sigma_param = F.softplus(pyro.param(\"inputb_sigma\", inputb_sigma))\n",
    "    inputb_prior = Normal(loc=inputb_mu_param, scale=inputb_sigma_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    outputw_mu = torch.randn_like(net.output.weight)\n",
    "    outputw_sigma = torch.randn_like(net.output.weight)\n",
    "    outputw_mu_param = pyro.param(\"outputw_mu\", outputw_mu)\n",
    "    outputw_sigma_param = F.softplus(pyro.param(\"outputw_sigma\", outputw_sigma))\n",
    "    outputw_prior = Normal(loc=outputw_mu_param, scale=outputw_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    outputb_mu = torch.randn_like(net.output.bias)\n",
    "    outputb_sigma = torch.randn_like(net.output.bias)\n",
    "    outputb_mu_param = pyro.param(\"outputb_mu\", outputb_mu)\n",
    "    outputb_sigma_param = F.softplus(pyro.param(\"outputb_sigma\", outputb_sigma))\n",
    "    outputb_prior = Normal(loc=outputb_mu_param, scale=outputb_sigma_param)\n",
    "    \n",
    "    priors = {'input.weight': inputw_prior, 'input.bias': inputb_prior, 'output.weight': outputw_prior, 'output.bias': outputb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimiser in this case ADAM and we are going to use the double stochastic VI and maximise the ELBO\n",
    "optim = pyro.optim.Adam({\"lr\": 0.1})\n",
    "svi = pyro.infer.svi.SVI(model, guide, optim, loss=pyro.infer.trace_elbo.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Bayesian NN functions thanks to sampling the models\n",
    "# we need to sample the model several times to get several different estimates\n",
    "# for the input data x \n",
    "num_samples = 10\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    std = torch.std(torch.stack(yhats), 0).numpy()\n",
    "    mean = np.argmax(mean.numpy(), axis=1)\n",
    "    std = np.array([std[i,mean[i]] for i in range(len(mean))])\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a utility function to calculate the accuracy\n",
    "def accuracy(out, labels):\n",
    "    return np.sum(out==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "train_std = []\n",
    "val_std = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    t,t_std = predict(train_dataset.tensors[0])\n",
    "    v,v_std = predict(val_dataset.tensors[0])\n",
    "    t_loss = svi.evaluate_loss(train_dataset.tensors[0].to(DEVICE), train_dataset.tensors[1].to(DEVICE))\n",
    "    v_loss = svi.evaluate_loss(val_dataset.tensors[0].to(DEVICE), val_dataset.tensors[1].to(DEVICE))\n",
    "    train_loss.append(t_loss)\n",
    "    val_loss.append(v_loss)\n",
    "    train_acc.append(accuracy(t,train_dataset.tensors[1].numpy()))\n",
    "    val_acc.append(accuracy(v,val_dataset.tensors[1].numpy()))\n",
    "    train_std.append(np.mean(t_std))\n",
    "    val_std.append(np.mean(v_std))\n",
    "    print(\"Training accuracy: {0:.2f}\".format(train_acc[-1]))\n",
    "    print(\"Validation accuracy: {0:.2f}\".format(val_acc[-1]))\n",
    "\n",
    "    # The main training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "        loss = svi.step(inputs, labels)\n",
    "\n",
    "        running_loss += loss\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 5))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        t,t_std = predict(train_dataset.tensors[0].to(DEVICE))\n",
    "        v,v_std = predict(val_dataset.tensors[0].to(DEVICE))\n",
    "        t_loss = svi.evaluate_loss(train_dataset.tensors[0].to(DEVICE), train_dataset.tensors[1].to(DEVICE))\n",
    "        v_loss = svi.evaluate_loss(val_dataset.tensors[0].to(DEVICE), val_dataset.tensors[1].to(DEVICE))\n",
    "        train_loss.append(t_loss)\n",
    "        val_loss.append(v_loss)\n",
    "        train_acc.append(accuracy(t,train_dataset.tensors[1].numpy()))\n",
    "        val_acc.append(accuracy(v,val_dataset.tensors[1].numpy()))\n",
    "        train_std.append(t_std.mean())\n",
    "        val_std.append(v_std.mean())\n",
    "        print(\"Training accuracy: {0:.2f}\".format(train_acc[-1]))\n",
    "        print(\"Validation accuracy: {0:.2f}\".format(val_acc[-1]))\n",
    "        \n",
    "print('Finished Training')\n",
    "with torch.no_grad():\n",
    "    t,t_std = predict(test_dataset.tensors[0])\n",
    "    print(\"Test accuracy: {0:.2f}\".format(accuracy(t,test_dataset.tensors[1].numpy())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss, accuracy as well as the standard deviation\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(train_loss,label=\"Train Loss\")\n",
    "ax1.plot(val_loss,label=\"Validation Loss\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(train_acc,linestyle='dashed',label=\"Train Accuracy\")\n",
    "ax2.plot(val_acc,linestyle='dashed',label=\"Validation Accuracy\")\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax2.legend()\n",
    "\n",
    "plt.title(\"Loss/Accuracy\")\n",
    "fig.tight_layout()\n",
    "\n",
    "print(train_std,val_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
